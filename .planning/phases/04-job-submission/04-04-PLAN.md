# Plan 04-04: Integration Tests

---
phase: 04-job-submission
plan: 04
type: execute
wave: 3
depends_on: ["04-01", "04-02", "04-03"]
files_modified:
  - python/crystalmath/quacc/mock_runner.py
  - python/tests/test_job_submission.py
  - tests/job_submission_integration.rs
autonomous: true

must_haves:
  truths:
    - "Job submission flow can be tested without real workflow engine"
    - "Python handlers work correctly with mock runner"
    - "Rust TUI models serialize/deserialize correctly with Python responses"
    - "Error paths are tested (missing POTCAR, engine not configured)"
  artifacts:
    - path: "python/crystalmath/quacc/mock_runner.py"
      provides: "MockRunner for testing"
      exports: ["MockRunner"]
    - path: "python/tests/test_job_submission.py"
      provides: "Handler tests with mocks"
      contains: "test_jobs_submit"
    - path: "tests/job_submission_integration.rs"
      provides: "Rust integration tests for job RPC"
      contains: "test_jobs_submit"
  key_links:
    - from: "python/tests/test_job_submission.py"
      to: "python/crystalmath/quacc/mock_runner.py"
      via: "MockRunner injection"
      pattern: "MockRunner"
---

<objective>
Create comprehensive tests for the job submission flow, using mock runners to avoid requiring real Parsl/Covalent/SLURM.

Purpose: Ensure job submission, status polling, and error handling work correctly before deploying to real HPC.

Output: Mock runner class, Python handler tests, Rust integration tests.
</objective>

<execution_context>
@/Users/briansquires/.claude/get-shit-done/workflows/execute-plan.md
@/Users/briansquires/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-job-submission/04-RESEARCH.md
@python/crystalmath/quacc/runner.py
@python/crystalmath/server/handlers/jobs.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Mock Runner</name>
  <files>python/crystalmath/quacc/mock_runner.py</files>
  <action>
Create a MockRunner for testing that simulates job lifecycle:

```python
"""Mock runner for testing job submission without real workflow engines."""

import uuid
from enum import Enum
from typing import Any

from crystalmath.quacc.runner import JobRunner, JobState


class MockJobState(str, Enum):
    """Internal state for mock jobs."""
    SUBMITTED = "submitted"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class MockRunner(JobRunner):
    """
    Mock runner for testing.

    Jobs follow a predictable lifecycle:
    - submit() -> PENDING
    - First get_status() -> RUNNING
    - Second get_status() -> COMPLETED (or FAILED if fail_job_ids contains job)

    This allows testing without Parsl/Covalent dependencies.
    """

    def __init__(self):
        self._jobs: dict[str, MockJobState] = {}
        self._status_calls: dict[str, int] = {}
        self._results: dict[str, dict] = {}
        # Job IDs that should fail
        self.fail_job_ids: set[str] = set()
        # Custom results per job
        self.custom_results: dict[str, dict] = {}

    def submit(
        self,
        recipe_fullname: str,
        atoms: Any,
        cluster_name: str,
        **kwargs,
    ) -> str:
        """Submit mock job, returns UUID."""
        job_id = str(uuid.uuid4())
        self._jobs[job_id] = MockJobState.SUBMITTED
        self._status_calls[job_id] = 0

        # Store mock result
        formula = "MockFormula"
        if hasattr(atoms, "get_chemical_formula"):
            formula = atoms.get_chemical_formula()

        self._results[job_id] = {
            "results": {"energy": -123.456, "forces": [[0.01, 0.02, 0.03]]},
            "formula_pretty": formula,
            "dir_name": f"/tmp/mock_job_{job_id}",
        }

        return job_id

    def get_status(self, job_id: str) -> JobState:
        """Get mock job status, advancing state each call."""
        if job_id not in self._jobs:
            raise ValueError(f"Unknown job: {job_id}")

        self._status_calls[job_id] += 1
        calls = self._status_calls[job_id]

        current = self._jobs[job_id]

        # State machine: SUBMITTED -> RUNNING -> COMPLETED/FAILED
        if current == MockJobState.SUBMITTED:
            self._jobs[job_id] = MockJobState.RUNNING
            return JobState.RUNNING
        elif current == MockJobState.RUNNING:
            if job_id in self.fail_job_ids:
                self._jobs[job_id] = MockJobState.FAILED
                return JobState.FAILED
            else:
                self._jobs[job_id] = MockJobState.COMPLETED
                return JobState.COMPLETED
        elif current == MockJobState.COMPLETED:
            return JobState.COMPLETED
        elif current == MockJobState.FAILED:
            return JobState.FAILED
        elif current == MockJobState.CANCELLED:
            return JobState.CANCELLED

        return JobState.PENDING

    def get_result(self, job_id: str) -> dict | None:
        """Get mock result if complete."""
        if job_id not in self._jobs:
            raise ValueError(f"Unknown job: {job_id}")

        if self._jobs[job_id] == MockJobState.COMPLETED:
            if job_id in self.custom_results:
                return self.custom_results[job_id]
            return self._results.get(job_id)
        elif self._jobs[job_id] == MockJobState.FAILED:
            return {"error": "Mock job failed intentionally"}

        return None

    def cancel(self, job_id: str) -> bool:
        """Cancel mock job."""
        if job_id not in self._jobs:
            return False

        current = self._jobs[job_id]
        if current in (MockJobState.SUBMITTED, MockJobState.RUNNING):
            self._jobs[job_id] = MockJobState.CANCELLED
            return True
        return False

    # Test helpers
    def set_fail(self, job_id: str):
        """Mark job to fail on next status check."""
        self.fail_job_ids.add(job_id)

    def force_state(self, job_id: str, state: MockJobState):
        """Force job into specific state for testing."""
        self._jobs[job_id] = state
```

Update __init__.py to export MockRunner for tests.
  </action>
  <verify>
```bash
cd /Users/briansquires/CRYSTAL23/crystalmath
uv run python -c "from crystalmath.quacc.mock_runner import MockRunner; r = MockRunner(); print('MockRunner works')"
```
  </verify>
  <done>MockRunner simulates job lifecycle for testing without Parsl/Covalent</done>
</task>

<task type="auto">
  <name>Task 2: Python Handler Tests</name>
  <files>python/tests/test_job_submission.py</files>
  <action>
Create comprehensive tests for job handlers:

```python
"""Tests for job submission handlers."""

import pytest
from unittest.mock import patch, MagicMock
import asyncio

from crystalmath.quacc.mock_runner import MockRunner
from crystalmath.quacc.runner import JobState


class TestJobsSubmit:
    """Tests for jobs.submit handler."""

    @pytest.mark.asyncio
    async def test_submit_returns_job_id(self):
        """Successful submission returns job_id."""
        from crystalmath.server.handlers.jobs import handle_jobs_submit

        mock_runner = MockRunner()

        with patch("crystalmath.server.handlers.jobs.get_runner", return_value=mock_runner), \
             patch("crystalmath.server.handlers.jobs.get_workflow_engine", return_value="parsl"), \
             patch("crystalmath.server.handlers.jobs.validate_potcars", return_value=(True, None)):

            result = await handle_jobs_submit(None, {
                "recipe": "quacc.recipes.vasp.core.relax_job",
                "structure": "Si\n1.0\n5.43 0.0 0.0\n0.0 5.43 0.0\n0.0 0.0 5.43\nSi\n8\nDirect\n0.0 0.0 0.0\n",
                "cluster": "local",
            })

        assert result["job_id"] is not None
        assert result["status"] == "pending"
        assert result["error"] is None

    @pytest.mark.asyncio
    async def test_submit_no_workflow_engine(self):
        """Submission fails without workflow engine."""
        from crystalmath.server.handlers.jobs import handle_jobs_submit

        with patch("crystalmath.server.handlers.jobs.get_workflow_engine", return_value=None):
            result = await handle_jobs_submit(None, {
                "recipe": "quacc.recipes.vasp.core.relax_job",
                "structure": "Si\n1.0\n...",
            })

        assert result["job_id"] is None
        assert result["status"] == "error"
        assert "workflow engine" in result["error"].lower()

    @pytest.mark.asyncio
    async def test_submit_potcar_validation_fails(self):
        """Submission fails if POTCARs missing."""
        from crystalmath.server.handlers.jobs import handle_jobs_submit

        with patch("crystalmath.server.handlers.jobs.get_workflow_engine", return_value="parsl"), \
             patch("crystalmath.server.handlers.jobs.validate_potcars",
                   return_value=(False, "Missing POTCARs for: Si")):

            result = await handle_jobs_submit(None, {
                "recipe": "quacc.recipes.vasp.core.relax_job",
                "structure": "Si\n1.0\n5.43 0.0 0.0\n0.0 5.43 0.0\n0.0 0.0 5.43\nSi\n8\nDirect\n0.0 0.0 0.0\n",
            })

        assert result["job_id"] is None
        assert result["status"] == "error"
        assert "POTCAR" in result["error"]


class TestJobsStatus:
    """Tests for jobs.status handler."""

    @pytest.mark.asyncio
    async def test_status_returns_running(self):
        """Status returns running for active job."""
        from crystalmath.server.handlers.jobs import handle_jobs_status
        from crystalmath.quacc.store import JobStore, JobMetadata, JobStatus
        from datetime import datetime, timezone

        # Create mock job in store
        store = JobStore()
        job = JobMetadata(
            id="test-job-123",
            recipe="relax_job",
            status=JobStatus.pending,
            created_at=datetime.now(timezone.utc),
            updated_at=datetime.now(timezone.utc),
        )
        store.save_job(job)

        mock_runner = MockRunner()
        # Submit a job to the runner so it knows about it
        mock_runner._jobs["test-job-123"] = mock_runner._jobs.get("test-job-123", "submitted")

        with patch("crystalmath.server.handlers.jobs.get_runner", return_value=mock_runner), \
             patch("crystalmath.server.handlers.jobs.get_workflow_engine", return_value="parsl"):

            result = await handle_jobs_status(None, {"job_id": "test-job-123"})

        assert result["job_id"] == "test-job-123"
        # Status should be running (first get_status call advances from submitted)
        assert result["status"] in ("pending", "running")


class TestJobsCancel:
    """Tests for jobs.cancel handler."""

    @pytest.mark.asyncio
    async def test_cancel_running_job(self):
        """Cancel returns success for running job."""
        from crystalmath.server.handlers.jobs import handle_jobs_cancel

        mock_runner = MockRunner()
        job_id = mock_runner.submit("relax_job", MagicMock(), "local")

        with patch("crystalmath.server.handlers.jobs.get_runner", return_value=mock_runner), \
             patch("crystalmath.server.handlers.jobs.get_workflow_engine", return_value="parsl"):

            result = await handle_jobs_cancel(None, {"job_id": job_id})

        assert result["job_id"] == job_id
        assert result["cancelled"] is True
```

Add more edge case tests:
- Invalid structure format
- Job not found
- Cancel completed job (should fail)
  </action>
  <verify>
```bash
cd /Users/briansquires/CRYSTAL23/crystalmath
uv run pytest python/tests/test_job_submission.py -v
```
  </verify>
  <done>Python handler tests pass with MockRunner</done>
</task>

<task type="auto">
  <name>Task 3: Rust Integration Tests</name>
  <files>tests/job_submission_integration.rs</files>
  <action>
Create Rust integration tests for job RPC:

```rust
//! Integration tests for job submission RPC.
//!
//! These tests verify the Rust TUI can correctly serialize requests
//! and deserialize responses from the Python backend.

use crystalmath::models::{
    ApiResponse, JobSubmitRequest, JobSubmitResponse, JobStatusResponse,
    JobResultSummary, QuaccJobMetadata,
};

mod common;

#[test]
fn test_job_submit_request_serialization() {
    let request = JobSubmitRequest {
        recipe: "quacc.recipes.vasp.core.relax_job".to_string(),
        structure: "Si\n1.0\n5.43 0.0 0.0\n0.0 5.43 0.0\n0.0 0.0 5.43\nSi\n8\n".to_string(),
        cluster: Some("nersc".to_string()),
        params: serde_json::json!({"kpts": [4, 4, 4], "encut": 520}),
    };

    let json = serde_json::to_string(&request).unwrap();

    assert!(json.contains("quacc.recipes.vasp.core.relax_job"));
    assert!(json.contains("nersc"));
    assert!(json.contains("520"));
}

#[test]
fn test_job_submit_response_success() {
    let json = r#"{"job_id": "abc-123", "status": "pending", "error": null}"#;
    let response: JobSubmitResponse = serde_json::from_str(json).unwrap();

    assert_eq!(response.job_id, Some("abc-123".to_string()));
    assert_eq!(response.status, "pending");
    assert!(response.error.is_none());
}

#[test]
fn test_job_submit_response_error() {
    let json = r#"{"job_id": null, "status": "error", "error": "Missing POTCARs"}"#;
    let response: JobSubmitResponse = serde_json::from_str(json).unwrap();

    assert!(response.job_id.is_none());
    assert_eq!(response.status, "error");
    assert_eq!(response.error, Some("Missing POTCARs".to_string()));
}

#[test]
fn test_job_status_response_running() {
    let json = r#"{
        "job_id": "abc-123",
        "status": "running",
        "error": null,
        "result": null
    }"#;
    let response: JobStatusResponse = serde_json::from_str(json).unwrap();

    assert_eq!(response.job_id, "abc-123");
    assert_eq!(response.status, "running");
    assert!(response.result.is_none());
}

#[test]
fn test_job_status_response_completed() {
    let json = r#"{
        "job_id": "abc-123",
        "status": "completed",
        "error": null,
        "result": {
            "energy_ev": -123.456,
            "max_force_ev_ang": 0.012,
            "formula": "Si8",
            "work_dir": "/scratch/job_abc"
        }
    }"#;
    let response: JobStatusResponse = serde_json::from_str(json).unwrap();

    assert_eq!(response.status, "completed");
    let result = response.result.unwrap();
    assert_eq!(result.energy_ev, Some(-123.456));
    assert_eq!(result.formula, Some("Si8".to_string()));
}

#[test]
fn test_job_status_response_failed() {
    let json = r#"{
        "job_id": "abc-123",
        "status": "failed",
        "error": "SCF convergence failed",
        "result": null
    }"#;
    let response: JobStatusResponse = serde_json::from_str(json).unwrap();

    assert_eq!(response.status, "failed");
    assert_eq!(response.error, Some("SCF convergence failed".to_string()));
}

#[test]
fn test_api_response_wrapper() {
    // Test success response
    let json = r#"{"ok": true, "data": {"job_id": "abc-123", "status": "pending", "error": null}}"#;
    let response: ApiResponse<JobSubmitResponse> = serde_json::from_str(json).unwrap();

    assert!(response.ok);
    let data = response.into_result().unwrap();
    assert_eq!(data.job_id, Some("abc-123".to_string()));

    // Test error response
    let json = r#"{"ok": false, "error": {"code": "VALIDATION_ERROR", "message": "Invalid structure"}}"#;
    let response: ApiResponse<JobSubmitResponse> = serde_json::from_str(json).unwrap();

    assert!(!response.ok);
    let err = response.into_result().unwrap_err();
    assert!(err.contains("VALIDATION_ERROR"));
}

// Integration test with actual server (optional, requires server running)
#[test]
#[ignore = "requires running server"]
fn test_jobs_list_integration() {
    use crystalmath::ipc::IpcClient;

    let mut client = IpcClient::connect().expect("Failed to connect");

    let result = client.call("jobs.list", serde_json::json!({"limit": 10}))
        .expect("RPC call failed");

    // Should return jobs array
    assert!(result.get("jobs").is_some());
}
```
  </action>
  <verify>
```bash
cd /Users/briansquires/CRYSTAL23/crystalmath
cargo test job_submission --no-fail-fast
```
  </verify>
  <done>Rust integration tests verify serialization/deserialization matches Python</done>
</task>

</tasks>

<verification>
```bash
cd /Users/briansquires/CRYSTAL23/crystalmath

# Python tests pass
uv run pytest python/tests/test_job_submission.py -v

# Rust tests pass
cargo test job_submission

# Full test suite
uv run pytest python/tests/ -v
cargo test
```
</verification>

<success_criteria>
- [ ] MockRunner simulates complete job lifecycle
- [ ] MockRunner can force failures for error testing
- [ ] Python handler tests cover success and error paths
- [ ] Jobs.submit test verifies POTCAR validation called
- [ ] Jobs.status test verifies status polling
- [ ] Jobs.cancel test verifies cancellation
- [ ] Rust tests verify request serialization
- [ ] Rust tests verify response deserialization for all states
- [ ] ApiResponse wrapper tests pass
- [ ] All tests run without requiring real workflow engines
</success_criteria>

<output>
After completion, create `.planning/phases/04-job-submission/04-04-SUMMARY.md`
</output>
